{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Notebook A: Introduction to JAX\n",
    "\n",
    "(c) 2024 Addison Howe. This work is licensed under a [Creative Commons\n",
    "Attribution License CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/).\n",
    "All code contained herein is licensed under an [MIT\n",
    "license](https://opensource.org/licenses/MIT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*JAX is a Python library for accelerator-oriented array computation and program transformation, designed for high-performance numerical computing and large-scale machine learning.* (From the [JAX documentation](https://github.com/google/jax).)\n",
    " \n",
    "JAX offers automatic differentiation capabilities, similar to packages such as Pytorch and Tensorflow. \n",
    "It also offers just-in-time compilation for user-defined Python functions, meaning that your code will run faster, with less overhead from the Python interpreter.\n",
    "Again from the JAX Github: \"Compilation and automatic differentiation can be composed arbitrarily, so you can express sophisticated algorithms and get maximal performance without leaving Python.\"\n",
    "\n",
    "## Installing JAX\n",
    "\n",
    "Assuming that you've installed conda on your system, you can install JAX in your current conda environment by running `conda install jax` from the command line.\n",
    "For the purposes of this tutorial, we'll start by making a fresh conda environment. \n",
    "In the command line, run\n",
    "```bash\n",
    "conda create -n jaxenv python=3.10 numpy=1.26 matplotlib=3.8 jax=0.4 diffrax=0.5 ipykernel\n",
    "conda activate jaxenv\n",
    "```\n",
    "This will create a new environment, called `jaxenv`, that will contain the JAX-related packages we'll be using.\n",
    "Test that you can import the `jax` and `diffrax` libraries by running \n",
    "```bash\n",
    "python -c \"import jax; import diffrax; print('Success!')\"\n",
    "```\n",
    "Note that this installation should be fine for our purposes, since we'll be running all of our code on a CPU. \n",
    "However, if you want to use JAX for your own research, taking advantage of high-performance architectures such as GPUs, you'll want to take a look at the other [installation options](https://github.com/google/jax?tab=readme-ov-file#installation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAX Basics\n",
    "Now that we've successfully installed JAX, let's dive into some basic functionality.\n",
    "Feel free to check out the [JAX Quickstart](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html) page for additional information.\n",
    "\n",
    "JAX comes with its own \"version\" of numpy, which feels a lot like the standard numpy package.\n",
    "Pretty much everything we can do in numpy, we can also do in JAX.\n",
    "One important thing to note is that JAX by default works with single-precision data, i.e. arrays have float32 or int32 datatypes.\n",
    "For machine learning applications, this is fairly common, but if we want 64-bit data for scientific computing purposes, we can set an environment variable or change a configuration setting, as we do below.\n",
    "More details about this can be found in the [documentation](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#double-64bit-precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want double precision datatypes, this needs to be done at startup,\n",
    "# when you first import stuff from jax.\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# JAX's version of numpy\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jnp.array([1,2,3])  # create an array. Notice the datatype is int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the datatype of the array is `int64`. If you restart your kernel and comment out the `config.update(...)` line, you'll see that without it the array will be `int32` instead.\n",
    "Let's do some basic operations with arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x                    : [1. 2. 3.]\n",
      "y                    : [1. 0. 1.]\n",
      "x + y                : [2. 2. 4.]\n",
      "jnp.linalg.norm(y)   : 1.4142135623730951\n",
      "2*x + 3*y            : [5. 4. 9.]\n",
      "jnp.dot(x,y)         : 4.0\n"
     ]
    }
   ],
   "source": [
    "x = jnp.array([1, 2, 3], dtype=jnp.float64)  # explicitly request float64 data\n",
    "y = jnp.array([1, 0, 1], dtype=jnp.float64)\n",
    "\n",
    "print(\"x                    :\", x)\n",
    "print(\"y                    :\", y)\n",
    "print(\"x + y                :\", x + y)\n",
    "print(\"jnp.linalg.norm(y)   :\", jnp.linalg.norm(y))\n",
    "print(\"2*x + 3*y            :\", 2*x + 3*y)\n",
    "print(\"jnp.dot(x,y)         :\", jnp.dot(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other helpful carryovers from numpy that we'll make use..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.25 0.5  0.75 1.  ]\n",
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(jnp.linspace(0, 1, 5))\n",
    "print(jnp.ones([3,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation and Vecorization in JAX\n",
    "\n",
    "Two features that make JAX very powerful for scientific computing include its [automatic differentiation](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#the-autodiff-cookbook) and [vectorization](https://jax.readthedocs.io/en/latest/jax-101/03-vectorization.html#automatic-vectorization) capabilities.\n",
    "\n",
    "### Basic differentiation with `grad`\n",
    "\n",
    "We can take derivatives of scalar functions easily, using the `jax.grad` function. By default, `jax.grad` takes in a function `f(x, ...)` and returns another function, the derivative of the function `f` with respect to its first argument, `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from jax import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   f(0) = 0.0\n",
      "  f'(0) = 1.0\n",
      " f''(0) = -0.0\n",
      "f'''(0) = -1.0\n"
     ]
    }
   ],
   "source": [
    "f = jnp.sin  # f is the sine function, defined in jax.numpy\n",
    "\n",
    "fx = grad(jnp.sin)                # fx is the 1st derivative of sine\n",
    "fxx = grad(grad(jnp.sin))         # fxx is the 2nd derivative of sine\n",
    "fxxx = grad(grad(grad(jnp.sin)))  # fxxx is the 3rd derivative of sine\n",
    "\n",
    "# Warning: the inputs to these functions must be floats, not integers.\n",
    "# If you use int 0 instead of float 0. (with a period) an error will be thrown.\n",
    "\n",
    "print(f\"   f(0) = { f(0.) }\")\n",
    "print(f\"  f'(0) = { fx(0.) }\")\n",
    "print(f\" f''(0) = { fxx(0.) }\")\n",
    "print(f\"f'''(0) = { fxxx(0.) }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `grad` function to differentiate functions with respect to positional arguments.\n",
    "As an example, consider the function \n",
    "$$f(x,y) = xy^2$$\n",
    "with \n",
    "$$\n",
    "\\begin{align}\n",
    "    \\frac{\\partial}{\\partial x}f(x,y) &= y^2 \\\\\n",
    "    \\frac{\\partial}{\\partial y}f(x,y) &= 2xy\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " f = 18.0\n",
      "fx = 9.0\n",
      "fy = 12.0\n"
     ]
    }
   ],
   "source": [
    "def f(x, y):\n",
    "    return x*y*y\n",
    "\n",
    "fx = grad(f, 0)  # Derivative in the 0th argument (x). Same as grad(f).\n",
    "fy = grad(f, 1)  # Derivative in the 1st argument (y).\n",
    "\n",
    "x = 2.\n",
    "y = 3.\n",
    "\n",
    "print(f\" f = {f(x, y)}\")\n",
    "print(f\"fx = {fx(x, y)}\")\n",
    "print(f\"fy = {fy(x, y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often in computing we will deal with functions $f:\\mathbb{R}^n\\to\\mathbb{R}^m$, where $n$ and $m$ can be very large.\n",
    "In these cases, we'll often define our python functions in terms of inputs that we assume to be vectors, or arrays.\n",
    "For example, if we think of the function above as mapping a 2-vector to a scalar, $f:\\mathbb{R}^2\\to\\mathbb{R}$, we can rewrite it in python, treating its input `x` as a 2-vector $(x,y)$.\n",
    "Then, if we take the gradient of $f$ with respect to the first input, it should return the gradient *vector*, $(f_x, f_y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        f(x) = 18.0\n",
      "(grad(f))(x) = [ 9. 12.]\n"
     ]
    }
   ],
   "source": [
    "# Now `x` represents the 2-vector (x,y) \n",
    "def f(x):\n",
    "    return x[0] * x[1] * x[1]\n",
    "\n",
    "x = jnp.array([2., 3.])  # still use the point (x, y) = (2, 3)\n",
    "\n",
    "g = grad(f)\n",
    "\n",
    "print(f\"        f(x) = {f(x)}\")\n",
    "print(f\"(grad(f))(x) = {g(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing functions using `vmap`\n",
    "\n",
    "If you've used numpy before, you probably know that it is convenient for vectorized operations, meaning that if you define a function and use \"basic\" operations, you can often apply the function over an array of inputs, and it just works. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18.  4.  3.]\n"
     ]
    }
   ],
   "source": [
    "def f(x, y):\n",
    "    return x * y * y\n",
    "\n",
    "x = jnp.array([2., 1., 3.])\n",
    "y = jnp.array([3., 2., 1.])\n",
    "\n",
    "print(f(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we run into a problem if we try to do something like this with the gradient version of the function. \n",
    "The following code will produce an error..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encountered an error: Gradient only defined for scalar-output functions. Output had shape: (3,).\n"
     ]
    }
   ],
   "source": [
    "def f(x, y):\n",
    "    return x * y * y\n",
    "\n",
    "x = jnp.array([2., 1., 3.])\n",
    "y = jnp.array([3., 2., 1.])\n",
    "\n",
    "grad_f = grad(f, 1)  # take the partial derivative with respect to y.\n",
    "\n",
    "try:\n",
    "    grad_f(x, y)  # this raises an error!\n",
    "except TypeError as e:\n",
    "    print(\"Encountered an error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get around this, we can use the `jax.vmap` function to vectorize arbitrary functions.\n",
    "This is a very convenient and flexible feature, that allows us to vectorize over certain axes of certain inputs. \n",
    "For example, to achieve what we want above, we can vectorize the gradient function so that the vectorized version will take in two arrays of length $n$ and return an array of length $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.  4.  6.]\n"
     ]
    }
   ],
   "source": [
    "from jax import vmap\n",
    "\n",
    "def f(x, y):\n",
    "    return x * y * y\n",
    "\n",
    "x = jnp.array([2., 1., 3.])\n",
    "y = jnp.array([3., 2., 1.])\n",
    "\n",
    "grad_f = grad(f, 1)  # take the partial derivative with respect to y, as above.\n",
    "\n",
    "# Now we vectorize the gradient function.\n",
    "# The second argument 0 says to vectorize over the 0th axis of each input.\n",
    "vectorized_grad = vmap(grad_f, 0)\n",
    "\n",
    "print(vectorized_grad(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: \n",
    "Consider the function $f:\\mathbb{R}^3\\to\\mathbb{R}$ with\n",
    "$$f(x,y,z)=xy+z$$\n",
    "and define the python function `f(x)` where `x` represents the 3-vector $(x,y,z)$.\n",
    "\n",
    "1. Use `vmap` to vectorize the function, so that the vectorized version takes a 2D-array of shape `(n, 3)` and returns an array of shape `(n,)`.\n",
    "2. Use `vmap` and `grad` to construct a vectorized version of the gradient function $\\nabla f=(f_x, f_y, f_z)$. This function should take as input a 2d-array of shape `(n, 3)` and return a 2d-array of shape `(n, 3)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):  # `x`` is a 3-vector (x,y,z)\n",
    "    return x[0] * x[1] + x[2]  # f = xy+z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got result:\n",
      " Not Implemented!\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1.1: Vectorize the function f.\n",
    "\n",
    "vectorized_f = lambda x: \"Not Implemented!\"  # ignore this line\n",
    "\n",
    "#!-------------------------  TODO: YOUR CODE HERE  -------------------------!#\n",
    "# Construct the function `vectorized_f`.\n",
    "\n",
    "# vectorized_f = ...\n",
    "\n",
    "#!--------------------------------------------------------------------------!#\n",
    "\n",
    "# Input: Array of shape (4, 3)\n",
    "xs = jnp.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 3],\n",
    "    [1, 1, 1],\n",
    "    [2, 3, 1],\n",
    "], dtype=jnp.float64)\n",
    "\n",
    "result = vectorized_f(xs)\n",
    "print(\"Got result:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got result:\n",
      " Not Implemented!\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1.2: Vectorize the gradient of the function f.\n",
    "\n",
    "vectorized_grad_f = lambda x: \"Not Implemented!\"  # ignore this line\n",
    "\n",
    "#!-------------------------  TODO: YOUR CODE HERE  -------------------------!#\n",
    "# Construct the function `vectorized_grad_f`.\n",
    "\n",
    "# vectorized_grad_f = ...\n",
    "\n",
    "#!--------------------------------------------------------------------------!#\n",
    "\n",
    "# Input: Array of shape (4, 3)\n",
    "xs = jnp.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 3],\n",
    "    [1, 1, 1],\n",
    "    [2, 3, 1],\n",
    "], dtype=jnp.float64)\n",
    "\n",
    "result = vectorized_grad_f(xs)\n",
    "print(\"Got result:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2:\n",
    "\n",
    "Use `jax.grad` and `jax.vmap` to plot a function $f$ and its derivative $f'$ over an interval $x\\in[-1, 1]$.\n",
    "Compute arrays `ys` and `yprimes`, the value of the function and its derivative, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x**3  # Feel free to change the function\n",
    "\n",
    "xs = jnp.linspace(-1, 1, 1000)\n",
    "\n",
    "ys = 0. * xs        # placeholder for the function\n",
    "yprimes = 0. * xs   # placeholder for the derivative\n",
    "\n",
    "\n",
    "#!-------------------------  TODO: YOUR CODE HERE  -------------------------!#\n",
    "# Compute the arrays `ys` and `yprimes`\n",
    "\n",
    "# ys = ...\n",
    "# yprimes = ...\n",
    "\n",
    "#!--------------------------------------------------------------------------!#\n",
    "\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(10,4))\n",
    "\n",
    "ax1.plot(xs, ys)  # Plot the function\n",
    "ax1.set_xlabel(\"$x$\")\n",
    "ax1.set_ylabel(\"$f(x)$\")\n",
    "ax1.set_title(\"$f(x)$\")\n",
    "\n",
    "ax2.plot(xs, yprimes)  # Plot the derivative\n",
    "ax2.set_xlabel(\"$x$\")\n",
    "ax2.set_ylabel(\"$f'(x)$\")\n",
    "ax2.set_title(\"$f'(x)$\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More with Vector-Valued Functions: Jacobians\n",
    "\n",
    "Recall that the Jacobian matrix $J\\in\\mathbb{R}^{m\\times n}$ of a transformation $F:\\mathbb{R}^n\\to\\mathbb{R}^m$ is an $m\\times n$ matrix containing the partial derivatives of each component of $F$ with respect to each input. That is,\n",
    "$$J_{ij} = \\frac{\\partial F_i}{\\partial x_j}.$$\n",
    "\n",
    "For example, if \n",
    "$$F(x,y) = \\begin{bmatrix}  xy \\\\ xy^2 \\\\ x^2 y \\end{bmatrix}, \\;\\;\\; F:\\mathbb{R}^2\\to\\mathbb{R}^3$$\n",
    "then the Jacobian of $F$ is\n",
    "$$\n",
    "J(x,y) = \\begin{bmatrix} \n",
    "    y   & x \\\\\n",
    "    y^2 & 2xy \\\\\n",
    "    2xy & x^2\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "JAX offers two functions to compute the Jacobian, `jacfwd` and `jacrev`.\n",
    "Both functions will compute the Jacobian correctly, but `jacfwd` uses forward-mode automatic differentiation, while `jacrev` uses reverse-mode.\n",
    "This difference in implementation comes down to whether the Jacobian is computed column by column, or row by row. \n",
    "If the Jacobian is \"tall,\" as in the example above with $m > n$, then forward-mode is more efficient.\n",
    "If the Jacobian is \"wide,\" with more columns than rows, $n\\gg m$, then reverse-mode is more efficient.\n",
    "For more details regarding the specific implementation of these functions, and the mathematics behind them, refer to the [documentation](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#jacobians-and-hessians-using-jacfwd-and-jacrev).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 1.]\n",
      " [4. 4.]\n",
      " [4. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from jax import jacfwd, jacrev\n",
    "\n",
    "def F(x):\n",
    "    return jnp.array([ \n",
    "        x[0] * x[1],            # xy\n",
    "        x[0] * x[1] * x[1],     # x y^2\n",
    "        x[0] * x[0] * x[1],     # x^2 y\n",
    "    ])\n",
    "\n",
    "x = jnp.array([1., 2.])\n",
    "\n",
    "J = jacfwd(F)\n",
    "\n",
    "print(J(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Number Generation in JAX\n",
    "\n",
    "The computational performance that JAX offers requires that all functions be deterministic. \n",
    "One consequence of this is that random number generation feels a little different in JAX than in, for example, numpy and scipy.\n",
    "To inject \"randomness\" into a program, we start by making a psuedo-random number generator `key`.\n",
    "This can be thought of as a seed for any \"random\" function we want to use.\n",
    "Each time we want to call such a function, we first \"split\" the key into two subkeys.\n",
    "One subkey is retained and becomes our new `key` to use down the road.\n",
    "The other subkey we pass into the random function.\n",
    "\n",
    "To demonstrate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our initial key: [ 0 42]\n",
      "Split the key into key and subkey...\n",
      "   key is now: [2465931498 3679230171]\n",
      "subkey is now: [255383827 267815257]\n",
      "\n",
      "Generate data with subkey...\n",
      "x: [[-0.55338771  0.94428308  0.14538109]]\n",
      "\n",
      "Generate data with the same subkey again...\n",
      "y: [[-0.55338771  0.94428308  0.14538109]]\n"
     ]
    }
   ],
   "source": [
    "import jax.random as jrandom\n",
    "\n",
    "key = jrandom.PRNGKey(seed=42)  # create a key, (looks like a pair of integers)\n",
    "print(\"Our initial key:\", key)\n",
    "\n",
    "print(\"Split the key into key and subkey...\")\n",
    "key, subkey = jrandom.split(key)  # split the key into 2 parts: key and subkey\n",
    "print(\"   key is now:\", key)\n",
    "print(\"subkey is now:\", subkey)\n",
    "\n",
    "print(\"\\nGenerate data with subkey...\")\n",
    "x = jrandom.normal(subkey, shape=[1, 3])  # use subkey to generate Gaussian data\n",
    "print(\"x:\", x)\n",
    "\n",
    "print(\"\\nGenerate data with the same subkey again...\")\n",
    "y = jrandom.normal(subkey, shape=[1, 3])  # Get the same data!\n",
    "print(\"y:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The takeaway is to not reuse `subkey`. Instead, after each use, split `key` to get a new `subkey`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our initial key: [ 0 42]\n",
      "Split the key into key and subkey...\n",
      "   key is now: [2465931498 3679230171]\n",
      "subkey is now: [255383827 267815257]\n",
      "\n",
      "Generate data with subkey...\n",
      "x: [[-0.55338771  0.94428308  0.14538109]]\n",
      "\n",
      "Split the key into key and subkey again...\n",
      "   key is now: [3164236999 3984487275]\n",
      "subkey is now: [3923418436 1366451097]\n",
      "\n",
      "Generate data with the new subkey...\n",
      "y: [[-0.32761323 -0.40663464  0.89080702]]\n"
     ]
    }
   ],
   "source": [
    "key = jrandom.PRNGKey(seed=42)  # create a key, which looks like 2 numbers\n",
    "print(\"Our initial key:\", key)\n",
    "\n",
    "print(\"Split the key into key and subkey...\")\n",
    "key, subkey = jrandom.split(key)  # split the key into 2 parts: key and subkey\n",
    "print(\"   key is now:\", key)\n",
    "print(\"subkey is now:\", subkey)\n",
    "\n",
    "print(\"\\nGenerate data with subkey...\")\n",
    "x = jrandom.normal(subkey, shape=[1, 3])  # use subkey to generate Gaussian data\n",
    "print(\"x:\", x)\n",
    "\n",
    "print(\"\\nSplit the key into key and subkey again...\")\n",
    "key, subkey = jrandom.split(key)  # split the key into 2 parts: key and subkey\n",
    "print(\"   key is now:\", key)\n",
    "print(\"subkey is now:\", subkey)\n",
    "\n",
    "print(\"\\nGenerate data with the new subkey...\")\n",
    "y = jrandom.normal(subkey, shape=[1, 3]) \n",
    "print(\"y:\", y)  # y is different from x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JIT Compilation: Making code faster\n",
    "\n",
    "One reason to use JAX is that it allows you to take advantage of the ease and convenience of Python, while offering performance increases that are usually not available due to the overhead of the Python interpreter, which executes python code line by line.\n",
    "\n",
    "The way JAX does this is through just-in-time (JIT) compilation.\n",
    "The function `jax.jit` takes in a function and returns a compiled, or \"jitted,\" version.\n",
    "The first time a jitted function is called, its \"instructions\" are essentially saved, and future calls to the function are faster (sometimes *much* faster).\n",
    "For more information , take a look at the [documentation](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#using-jit-to-speed-up-functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "\n",
    "# Example from https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#using-jit-to-speed-up-functions\n",
    "\n",
    "# Define a somewhat complicated function\n",
    "def myfunc(x, alpha=1.67, lmbda=1.05):\n",
    "    return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "key, subkey = jrandom.split(key)\n",
    "x = jrandom.normal(subkey, (1000000,))  # make a large array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time the non-compiled function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.45 ms ± 298 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 100 myfunc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply `jit` to the function, and time the jitted version..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfunc_jit = jit(myfunc)  # `myfunc_jit` is a function that acts just like `myfunc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ms ± 60.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 100 myfunc_jit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the two will likely vary on different computers, but you should see that the jitted version is faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we learned some basics of JAX, including vectorization, random number generation, and jitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.9.19\n",
      "IPython version      : 8.18.1\n",
      "\n",
      "jax       : 0.4.26\n",
      "diffrax   : 0.5.1\n",
      "numpy     : 1.26.4\n",
      "matplotlib: 3.8.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p jax,diffrax,numpy,matplotlib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
